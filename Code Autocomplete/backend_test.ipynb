{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2534cb41-2cf2-486a-b07a-8d143be8eab2",
   "metadata": {},
   "source": [
    "# App is good to go! start it with `streamlit run app.py` but we are to test the backend first"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b344f5e4",
   "metadata": {},
   "source": [
    "we are but to follow the same, right steps as we did for Auto_complete_psk.ipynb to test the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04b22b8-60a7-4d7a-99b4-213365c34d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5186ca72-4f42-471e-b5e4-81989489ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dc6ae62-a41c-4dd3-af43-03835b08eaea",
   "metadata": {},
   "source": [
    "### Loading Vocab ( of course all that tokenizing and stuff is doen and we just have to load the path to the vocab object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b859620-4793-460d-9797-1ca1bb0dfe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = torch.load('vocab_obj.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff43a50-5519-4518-b500-7abec2465fee",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14f6f7f1-cb51-415e-af4a-4c4bfe2c8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        #embedding: [batch size, seq len, emb_dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)      \n",
    "        #output: [batch size, seq len, hid_dim]\n",
    "        #hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        #prediction: [batch size, seq_len, vocab size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f35ba7-b6c7-493f-8a2b-f742623151df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024\n",
    "hid_dim = 1024\n",
    "num_layers = 2\n",
    "dropout_rate = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b588cff3-20d9-46f1-ae4f-83d0bdf3b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2aa470-16e1-4b06-b8be-71ebe13c9d30",
   "metadata": {},
   "source": [
    "### Loading Learned Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ca166cb-94b3-4f67-8957-8b33419aadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './models/best-val-auto.pt' # <--------------- Change file path before submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d558444-b981-47e4-bed4-5f062ce28dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c32fb396-2879-457c-90ed-e5ccc0ac8e04",
   "metadata": {},
   "source": [
    "# Inference/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee01f1f7-1feb-496b-8801-9900336d16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec7de01-602c-4b1a-9bd8-1bcb88c18aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_name = tokenize.tok_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad73d8f6-336c-4a39-a421-a1b7cfa52da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_code_tokenizer(content):\n",
    "    tokenized_code = []\n",
    "    \n",
    "    try:\n",
    "        for token in tokenize.generate_tokens(io.StringIO(content).readline):\n",
    "            encoding = tok_name[token.type]\n",
    "            if encoding == \"COMMENT\" or encoding== \"NL\":\n",
    "                continue\n",
    "            elif encoding == \"NUMBER\":\n",
    "                tokenized_code.append(\"<NUMBER>\")\n",
    "            elif encoding == \"STRING\":\n",
    "                tokenized_code.append(\"<STRING>\")\n",
    "            else:\n",
    "                tokenized_code.append(token.string)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    return tokenized_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d83f7913-91ca-4ab8-9243-3d00bf209b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "902bffe6-e9de-4fb6-ad06-abb25025457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "import   def run_task ( ) : \n",
      "      import importlib \n",
      "\n",
      "\n",
      "0.7\n",
      "import   def samp_traj ( raw_data , w , len ( backbone\n",
      "\n",
      "0.75\n",
      "import   def samp_traj ( raw_data , w , len ( backbone\n",
      "\n",
      "0.8\n",
      "import   def samp_traj ( raw_data , w , len ( backbone\n",
      "\n",
      "1.0\n",
      "import   def samp_traj ( raw_data = decoded_preds , sketch_feat = t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'import'\n",
    "max_seq_len = 10\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, python_code_tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e14e040-d619-4dc7-9e3f-80df42445bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import torch . nn   if torch . cuda . is_available ( ) and not os . path . isdir ( path ) and args . outf and not os . path . isdir (\n",
      "\n",
      "import torch . nn   if torch . nn . Parameter ( torch . tensor ) , <STRING> : \n",
      "              if not self . cuda : \n",
      "              assert isinstance ( x , list\n",
      "\n",
      "import torch . nn   if torch . nn . Parameter ( torch . tensor ) , <STRING> : \n",
      "              if not self . cuda : \n",
      "              assert isinstance ( model , nn\n",
      "\n",
      "import torch . nn   if torch . nn . Parameter ( torch . tensor ) , <STRING> : \n",
      "              if not self . cuda : \n",
      "              assert state . dtype in storage\n",
      "\n",
      "import torch . nn   if torch . nn . Parameter ( torch . tensor ) : \n",
      "              import pytest \n",
      " from collections import random \n",
      " import numpy as np \n",
      " import pytest \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'import torch.nn'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, python_code_tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(' '.join(generation), end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a59b473075a889197cef78f691a8dde253fc9cd06ebdea22432c59d124001e4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
