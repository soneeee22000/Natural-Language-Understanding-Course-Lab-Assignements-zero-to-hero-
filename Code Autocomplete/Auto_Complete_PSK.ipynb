{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90bbe30-b3bf-4d0e-978f-c04fecade210",
   "metadata": {},
   "source": [
    "# Code Autocomplete "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c669493-03e4-4bde-be9f-ff0f1c83b6f1",
   "metadata": {},
   "source": [
    "<b>Name:</b> Pyae Sone Kyaw  <b>Student Id:</b> st123225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d48812-b15d-4818-b760-c9275d0d97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext, datasets, math\n",
    "from torchtext.vocab import vocab as torchTextVocab\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eecd490-74e3-40aa-b685-389edd7979a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcf7c88f-9b1b-4e44-ac86-0deedcf84e6c",
   "metadata": {},
   "source": [
    "# 1) First thing first , Load Dataset as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1667ed08-abf9-4537-ad48-a992bfa4c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lvwerra--codeparrot-clean-fb728533b9673c8b\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"lvwerra/codeparrot-clean\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bad96f0-5b74-4526-9698-5af16578a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dataset = iter(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9382daab",
   "metadata": {},
   "source": [
    "why iter? because it is a generator and it is memory efficient! represents an iterable over data samples. particularly suitable for cases where random reads are expensive, such as reading a large dataset from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087f702d-f20d-43e3-bc82-1d911d0279fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['repo_name',\n",
       " 'path',\n",
       " 'copies',\n",
       " 'size',\n",
       " 'content',\n",
       " 'license',\n",
       " 'hash',\n",
       " 'line_mean',\n",
       " 'line_max',\n",
       " 'alpha_frac',\n",
       " 'autogenerated']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(next(iter_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b50074-e764-4f74-9600-b61edd6bbff1",
   "metadata": {},
   "source": [
    "# 2) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f94f57-f02b-4eaf-8e35-6e57357b4110",
   "metadata": {},
   "source": [
    "### Extracting data related to pytorch code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fa5dd43-740b-4248-a509-9a788ac73441",
   "metadata": {},
   "source": [
    "Since our goal is to extract code related to Python code, we are going to do that by extracting the first 1000 repo suspected to contain Pytorch code. \n",
    "However, we also need to know that other libraries are also used in Pytorch code. So, we are going to extract the first 1000 repo suspected to contain Pytorch code and also the first 1000 repo suspected to contain other libraries used in Pytorch code. This is done by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d18872-4dc6-4f77-b576-81e71d4904de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_related = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb581de0-6da8-4711-adb7-6c45c18d1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "threshold = 1000\n",
    "count = 0\n",
    "\n",
    "for repo in iter_dataset:\n",
    "    if 'torch' in repo['repo_name']:\n",
    "        pytorch_related.append(repo)\n",
    "        count += 1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        \n",
    "        if count == threshold:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb3b9c8-81de-4b47-aebb-69fa5831aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/pytorch-transformers\n",
      "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import unittest\n",
      "\n",
      "import pytest\n",
      "\n",
      "from transformers import pipeline\n",
      "from transformers.testing_utils import is_pipeline_test, is_torch_available, require_torch, slow\n",
      "\n",
      "from .test_pipelines_common import MonoInputPipelineCommonMixin\n",
      "\n",
      "\n",
      "if is_torch_available():\n",
      "    from transformers.models.mbart import MBart50TokenizerFast, MBartForConditionalGeneration\n",
      "\n",
      "\n",
      "class TranslationEnToDePipelineTests(MonoInputPipelineCommonMixin, unittest.TestCase):\n",
      "    pipeline_task = \"translation_en_to_de\"\n",
      "    small_mo\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(pytorch_related[idx][\"repo_name\"])\n",
    "print(pytorch_related[idx][\"content\"][:1100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc2f29fa-7520-4cb0-ac50-53349e00da1e",
   "metadata": {},
   "source": [
    "### Creating Vocabulary - Tokenizing First 1000 Repo\n",
    "of course, to tokenize the code, we need to create a vocabulary. We are going to use a simple tokenize function to create a vocab. it's meant to provide a lexical scanner for Python code. However, in this case, the module was extremely useful since apart from returning tokens, it also return its type which made it possible to treat numbers and strings alike. Also during tokenizing, comments were excluded since they are not useful for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f6cfec1-f4c0-4e54-9ed9-b805097361d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c669b9c4-1210-47da-86c8-fe0bed0aaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_name = tokenize.tok_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcf65397-5745-4fa0-b909-43ef68a58124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'ENDMARKER', 1: 'NAME', 2: 'NUMBER', 3: 'STRING', 4: 'NEWLINE', 5: 'INDENT', 6: 'DEDENT', 7: 'LPAR', 8: 'RPAR', 9: 'LSQB', 10: 'RSQB', 11: 'COLON', 12: 'COMMA', 13: 'SEMI', 14: 'PLUS', 15: 'MINUS', 16: 'STAR', 17: 'SLASH', 18: 'VBAR', 19: 'AMPER', 20: 'LESS', 21: 'GREATER', 22: 'EQUAL', 23: 'DOT', 24: 'PERCENT', 25: 'LBRACE', 26: 'RBRACE', 27: 'EQEQUAL', 28: 'NOTEQUAL', 29: 'LESSEQUAL', 30: 'GREATEREQUAL', 31: 'TILDE', 32: 'CIRCUMFLEX', 33: 'LEFTSHIFT', 34: 'RIGHTSHIFT', 35: 'DOUBLESTAR', 36: 'PLUSEQUAL', 37: 'MINEQUAL', 38: 'STAREQUAL', 39: 'SLASHEQUAL', 40: 'PERCENTEQUAL', 41: 'AMPEREQUAL', 42: 'VBAREQUAL', 43: 'CIRCUMFLEXEQUAL', 44: 'LEFTSHIFTEQUAL', 45: 'RIGHTSHIFTEQUAL', 46: 'DOUBLESTAREQUAL', 47: 'DOUBLESLASH', 48: 'DOUBLESLASHEQUAL', 49: 'AT', 50: 'ATEQUAL', 51: 'RARROW', 52: 'ELLIPSIS', 53: 'COLONEQUAL', 54: 'OP', 55: 'AWAIT', 56: 'ASYNC', 57: 'TYPE_IGNORE', 58: 'TYPE_COMMENT', 59: 'ERRORTOKEN', 60: 'COMMENT', 61: 'NL', 62: 'ENCODING', 63: 'N_TOKENS', 256: 'NT_OFFSET'}\n"
     ]
    }
   ],
   "source": [
    "print(tok_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e1e637-1647-4c68-b8f6-0447abb111f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_code_tokenizer(content):\n",
    "    tokenized_code = []\n",
    "    \n",
    "    try:\n",
    "        for token in tokenize.generate_tokens(io.StringIO(content).readline):\n",
    "            encoding = tok_name[token.type]\n",
    "            if encoding == \"COMMENT\" or encoding== \"NL\":\n",
    "                continue\n",
    "            elif encoding == \"NUMBER\":\n",
    "                tokenized_code.append(\"<NUMBER>\")\n",
    "            elif encoding == \"STRING\":\n",
    "                tokenized_code.append(\"<STRING>\")\n",
    "            else:\n",
    "                tokenized_code.append(token.string)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    return tokenized_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d811310a-63b6-4521-8d7b-ae0038604aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_code_list = []\n",
    "\n",
    "for code in pytorch_related:\n",
    "    tokenized_code = python_code_tokenizer(code[\"content\"])\n",
    "    \n",
    "    if len(tokenized_code) > 0:\n",
    "        tokenized_code_list.append(tokenized_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "741ba948-8283-4737-9142-a8bbe585894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import', 'unittest', '\\n', 'import', 'pytest', '\\n', 'from', 'transformers', 'import', 'pipeline', '\\n', 'from', 'transformers', '.', 'testing_utils', 'import', 'is_pipeline_test', ',', 'is_torch_available', ',']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_code_list[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cb7188-2903-472b-8e79-f110e44979e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_code_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d025f99-64ac-428e-b048-678a77dcb955",
   "metadata": {},
   "source": [
    "### Splitting datasets into train,valid and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e03cf1-ebec-4f2a-9611-55ddcf19983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Len = len(tokenized_code_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dd35463-20b8-4f07-9f52-43e5e81e86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "end   = int(0.7 * Len)\n",
    "train = tokenized_code_list[:end]\n",
    "\n",
    "start = int(0.7 * Len)\n",
    "end   = int(0.8 * Len)\n",
    "valid = tokenized_code_list[start: end]\n",
    "\n",
    "start = int(0.8 * Len)\n",
    "test  = tokenized_code_list[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b56a570d-1599-4f59-a283-58967423cb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 100, 200)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34aa3171-e9e7-4704-996f-b70081dec7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_Dataset(dataset, feature_name):\n",
    "    my_list = []\n",
    "    \n",
    "    for data in dataset:\n",
    "        my_list.append({feature_name: data})\n",
    "    \n",
    "    return Dataset.from_list(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e46b873c-3014-4020-8ec7-eb95502a9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = convert_to_Dataset(train, \"tokens\")\n",
    "val_dataset   = convert_to_Dataset(valid, \"tokens\")\n",
    "test_dataset  = convert_to_Dataset(test, \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74a6411c-4d5e-448c-9a32-5116794ed625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens'],\n",
      "    num_rows: 699\n",
      "}) Dataset({\n",
      "    features: ['tokens'],\n",
      "    num_rows: 100\n",
      "}) Dataset({\n",
      "    features: ['tokens'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e739f1-016b-43c5-a5cc-ca58f5cd9a8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0287f24f-114a-4666-858e-7aeb10328ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_code = [item for sublist in train_dataset[\"tokens\"] for item in sublist]\n",
    "# vocab = list(set(flattened_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d7b1ccd-411f-4bc8-9137-e583f505858f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.insert(0, '<unk>')\n",
    "# vocab.insert(1, '<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e47e3e17-0b4a-476d-a9ff-9384632c0da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e6be30d-e2db-41c4-bfdd-dd2dce09af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d54bdbbd-b171-4f14-b24c-878acd5ada5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = Counter(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7587c21-a019-4c85-85fb-158c27c682cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter_dict = OrderedDict(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61891da4-786c-4165-a870-7d16f9842ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = Vocab(counter_dict)\n",
    "# vocab.set_default_index(vocab['<unk>'])\n",
    "# print(len(vocab))                         \n",
    "# print(vocab.get_itos()[:10])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3165789a-b30f-41d7-92a7-eb63a8bf218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24900\n",
      "['<unk>', '<eos>', '\\n', '.', ',', '(', ')', '=', '<STRING>', ':']\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(train_dataset['tokens']) \n",
    "vocab.insert_token('<unk>', 0)           \n",
    "vocab.insert_token('<eos>', 1)            \n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:10])      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a63f1e9f-61fb-42d8-8f16-4b11e95a2f5b",
   "metadata": {},
   "source": [
    "#### Saving Vocab Object for future use in web app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2c62928-213e-4191-b4a6-680afc9b2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab, 'vocab_obj.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f071df1c-e176-4f1b-a4b9-1c2bfe80b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_obj = torch.load('vocab_obj.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54c8cbd5-6b5a-4505-9053-a8cdd7f0d10d",
   "metadata": {},
   "source": [
    "# 3) Prepare Data loader for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "152b3f7c-dbc3-443c-ac3c-0adf3051d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:         \n",
    "            #appends eos so we know it ends....so model learn how to end...                             \n",
    "            tokens = example['tokens'].append('<eos>')   \n",
    "            #numericalize          \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size #get the int number of batches...\n",
    "    data = data[:num_batches * batch_size] #make the batch evenly, and cut out any remaining                      \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data #[batch size, bunch of tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3833f1a1-a44a-4c03-8525-34d7a2058058",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(train_dataset, vocab, batch_size)\n",
    "valid_data = get_data(val_dataset, vocab, batch_size)\n",
    "test_data  = get_data(test_dataset, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2647e-6fa7-48ad-8b57-0659759ba10e",
   "metadata": {},
   "source": [
    "# 4) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9a8f10e-c77b-42b5-ba06-537f8e8b12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim = hid_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
    "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "    \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        #embedding: [batch size, seq len, emb_dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)      \n",
    "        #output: [batch size, seq len, hid_dim]\n",
    "        #hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
    "        output = self.dropout(output) \n",
    "        prediction = self.fc(output)\n",
    "        #prediction: [batch size, seq_len, vocab size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99810760-4bc8-4ff2-8b64-f043cb85fb0f",
   "metadata": {},
   "source": [
    "# 5) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c4c58d61-7fcd-4d4b-9eca-473500aa605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 1024\n",
    "hid_dim = 1024\n",
    "num_layers = 2\n",
    "dropout_rate = 0.65              \n",
    "lr = 1e-3   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0d6635f5-b5f9-434d-a31e-d23a99a3e067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 67,813,700 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "771296f8-3069-48dc-9220-deb6ffad9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #this data is from get_data()\n",
    "    #train_data.shape # [batch_size, number of batches....]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9082ba4f-684b-42bc-ba80-e32c6cad0019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)               \n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) #prevent gradient explosion - clip is basically \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "be11dd43-ce72-4c25-9100-51a9ba4660d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0fdc13d6-f546-417d-9876-340087a25969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 1\n",
      "\tTrain Perplexity: 77.465\n",
      "\tValid Perplexity: 64.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 2\n",
      "\tTrain Perplexity: 18.981\n",
      "\tValid Perplexity: 17.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 3\n",
      "\tTrain Perplexity: 13.566\n",
      "\tValid Perplexity: 15.742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 4\n",
      "\tTrain Perplexity: 11.074\n",
      "\tValid Perplexity: 14.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 5\n",
      "\tTrain Perplexity: 9.491\n",
      "\tValid Perplexity: 13.856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 6\n",
      "\tTrain Perplexity: 8.325\n",
      "\tValid Perplexity: 13.241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 7\n",
      "\tTrain Perplexity: 7.456\n",
      "\tValid Perplexity: 13.185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 8\n",
      "\tTrain Perplexity: 6.751\n",
      "\tValid Perplexity: 12.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 9\n",
      "\tTrain Perplexity: 6.173\n",
      "\tValid Perplexity: 12.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 10\n",
      "\tTrain Perplexity: 5.713\n",
      "\tValid Perplexity: 12.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 11\n",
      "\tTrain Perplexity: 5.322\n",
      "\tValid Perplexity: 12.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 12\n",
      "\tTrain Perplexity: 4.976\n",
      "\tValid Perplexity: 12.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 13\n",
      "\tTrain Perplexity: 4.792\n",
      "\tValid Perplexity: 12.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 14\n",
      "\tTrain Perplexity: 4.652\n",
      "\tValid Perplexity: 12.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 15\n",
      "\tTrain Perplexity: 4.632\n",
      "\tValid Perplexity: 11.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 16\n",
      "\tTrain Perplexity: 4.555\n",
      "\tValid Perplexity: 11.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 17\n",
      "\tTrain Perplexity: 4.490\n",
      "\tValid Perplexity: 11.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 18\n",
      "\tTrain Perplexity: 4.506\n",
      "\tValid Perplexity: 11.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 19\n",
      "\tTrain Perplexity: 4.475\n",
      "\tValid Perplexity: 11.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 20\n",
      "\tTrain Perplexity: 4.539\n",
      "\tValid Perplexity: 11.383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 21\n",
      "\tTrain Perplexity: 4.522\n",
      "\tValid Perplexity: 11.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 22\n",
      "\tTrain Perplexity: 4.502\n",
      "\tValid Perplexity: 11.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 23\n",
      "\tTrain Perplexity: 4.582\n",
      "\tValid Perplexity: 11.265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 24\n",
      "\tTrain Perplexity: 4.557\n",
      "\tValid Perplexity: 11.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 25\n",
      "\tTrain Perplexity: 4.625\n",
      "\tValid Perplexity: 11.208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 26\n",
      "\tTrain Perplexity: 4.617\n",
      "\tValid Perplexity: 11.167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 27\n",
      "\tTrain Perplexity: 4.602\n",
      "\tValid Perplexity: 11.146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 28\n",
      "\tTrain Perplexity: 4.582\n",
      "\tValid Perplexity: 11.164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 29\n",
      "\tTrain Perplexity: 4.624\n",
      "\tValid Perplexity: 11.150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 30\n",
      "\tTrain Perplexity: 4.667\n",
      "\tValid Perplexity: 11.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 31\n",
      "\tTrain Perplexity: 4.656\n",
      "\tValid Perplexity: 11.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 32\n",
      "\tTrain Perplexity: 4.652\n",
      "\tValid Perplexity: 11.131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 33\n",
      "\tTrain Perplexity: 4.648\n",
      "\tValid Perplexity: 11.124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 34\n",
      "\tTrain Perplexity: 4.640\n",
      "\tValid Perplexity: 11.114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 35\n",
      "\tTrain Perplexity: 4.640\n",
      "\tValid Perplexity: 11.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 36\n",
      "\tTrain Perplexity: 4.638\n",
      "\tValid Perplexity: 11.098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 37\n",
      "\tTrain Perplexity: 4.633\n",
      "\tValid Perplexity: 11.090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 38\n",
      "\tTrain Perplexity: 4.626\n",
      "\tValid Perplexity: 11.080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 39\n",
      "\tTrain Perplexity: 4.623\n",
      "\tValid Perplexity: 11.075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 40\n",
      "\tTrain Perplexity: 4.618\n",
      "\tValid Perplexity: 11.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 41\n",
      "\tTrain Perplexity: 4.620\n",
      "\tValid Perplexity: 11.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 42\n",
      "\tTrain Perplexity: 4.615\n",
      "\tValid Perplexity: 11.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 43\n",
      "\tTrain Perplexity: 4.622\n",
      "\tValid Perplexity: 11.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 44\n",
      "\tTrain Perplexity: 4.631\n",
      "\tValid Perplexity: 11.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 45\n",
      "\tTrain Perplexity: 4.632\n",
      "\tValid Perplexity: 11.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 46\n",
      "\tTrain Perplexity: 4.631\n",
      "\tValid Perplexity: 11.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 47\n",
      "\tTrain Perplexity: 4.636\n",
      "\tValid Perplexity: 11.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 48\n",
      "\tTrain Perplexity: 4.634\n",
      "\tValid Perplexity: 11.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 49\n",
      "\tTrain Perplexity: 4.630\n",
      "\tValid Perplexity: 11.067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tepoch: 50\n",
      "\tTrain Perplexity: 4.628\n",
      "\tValid Perplexity: 11.067\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "seq_len  = 15\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './models/best-val-auto.pt')\n",
    "    print(f'\\tepoch: {epoch+1}')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de9d18c2-550f-4c9a-b1d8-3edf2a8d6da0",
   "metadata": {},
   "source": [
    "# 6) Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f9def2ae-0ebe-4a39-a7ad-d1ed0540c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 15.408\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./models/best-val-auto.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d697308-595f-47a8-a5dc-1e226b8819d1",
   "metadata": {},
   "source": [
    "# 7) Real Time Inference / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a93a17a0-021b-4eab-8366-be1a72d4e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "bb1817b1-a4f1-4c8e-8a52-fc99d84e062e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "import   def _score_sentence ( x , * args , ** kwargs\n",
      "\n",
      "0.7\n",
      "import   def _score_sentence ( d , * args , ** kwargs\n",
      "\n",
      "0.75\n",
      "import   def _score_sentence ( d , * args , ** kwargs\n",
      "\n",
      "0.8\n",
      "import   def _score_sentence ( d , * args , ** kwargs\n",
      "\n",
      "1.0\n",
      "import   from . test_generation_utils import LayoutLMTokenizerFast \n",
      " from . join import\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'import'\n",
    "max_seq_len = 10\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, python_code_tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a59b473075a889197cef78f691a8dde253fc9cd06ebdea22432c59d124001e4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
